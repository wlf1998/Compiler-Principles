# 实验报告：pl/0 语言的词法分析器
## 思路
词法分析的完整步骤应当是：给定正则表达式，生成 NFA，转为 DFA，化简 DFA，进行词法分析。

本实验中认为从正则表达式中提取 NFA 比较困难，所以直接从最简 DFA 开始，即手工作出几个最简 DFA。

由于是手工作出 DFA，所以分为四个 DFA：ID, 数字或非法 ID，符号，关键字。排在后面的 DFA 优先级更高。

## 数据结构
首先是 DFA 列表：
```
typedef struct _PL0DFAList
{
	int DFA_number;
	DFA** DFAs;
} DFAList;
```
然后是单个 DFA：
```
typedef struct _PL0DFA
{
	BOOL is_avaliable;                                   // 指示该 DFA 当前是否有效
	BOOL has_found_token;                                // 指示该 DFA 是否已经找到一个 TOKEN
	PL0TokenType token_found;                            // 标识已经找到的 TOKEN
	int current_state;
	int state_number;
	DFAState* states;                                    // 状态数组，其中状态所拥有的索引值即为索引 ID
} DFA;
```
然后是 DFA 拥有的单个状态：
```
typedef struct _PL0DFA_state
{
	int state_id;                                        // 状态 ID
	BOOL is_accepted_state;                              // 标识该状态是否为一个接受状态
	PL0TokenType target_token_type;                      // 如果它是一个接受状态，标识目标词法单元
	int validator_number;                                // 验证器个数
	BOOL(**validators)(char input, char param);          // 验证器数组，数组元素类型：函数指针
	char* params;                                        // 如果验证器需要一个额外参数，那么将这个数组内的对应元素传进去
	int* target_states;                                  // 与验证器数组配套的目标状态数组
} DFAState;
```
一个列表拥有多个 DFA，一个 DFA 拥有多个状态，一个状态拥有多个验证器，验证器即对应着每个状态能接受的输入。

这里还用到自己创建的文件读写数据结构：
```
typedef struct CDFileHelper
{
	FILE* file;
	int current_row;
	int current_column;
	int last_column;
} Cdfh;
```
FileHelper 可以记录当前行列，算是简单地封装文件读写与行列计算的逻辑。

经过修改的 pl0_lex 结构：
```
typedef struct _tPL0Lex {
	struct _tPL0Compiler * compiler;
	PL0TokenType last_token_type;
	char last_id[MAX_ID_LEN + 1];
	int last_num;

	int last_level;

	Cdfh* filehelper;
	DFAList* dfas;

	int last_start_row;
	int last_start_column;
	int last_len;
	
} PL0Lex;
```
## DFA
DFA 的数据被存储在文本文件内，词法分析器初始化时被读入，具体可以参考 lex 文件夹的相应文件。

## PL0Lex_get_token()
这个函数的逻辑其实比较简单。

每次读入一个字符，然后向有效的 DFA 输入该字符，然后检查是否有 DFA 已经标识为“已找到 TOKEN”的状态。

## 重要实现
主要的实现包括：

cdfh_ReadToSpace();

DFA_Load();

PL0Lex_get_token();

## 遇到的困难
对 DFA 的数据结构抽象花了一些时间，因为需要完整地回顾 DFA 的工作过程。

读入 DFA 的数据时有很多的内存分配过程，保证无越界访问是代码中的一个难点。

另外，get_token 的过程中，读入字符控制的逻辑也是有一些细节工作的，花了比较多的时间。